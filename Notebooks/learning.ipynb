{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In diesem Notebook lernen wir die Netzwerke an\n",
    "\n",
    "1. Trainingsdatenstruktur\n",
    "2. Generierung synthetischer Daten\n",
    "3. Training Netzwerke\n",
    "\n",
    "Da laut Jens Decker bei Bruker nur Randverteilungen (Summen über eine Achse) betrachtet werden, machen wir dies genauso und betrachten in jedem Netzwerk nur die Randverteilungen des Berges. Weiterhin ohne vernünftige Trainingsdaten (bzw. ohne eine wirkliche Verteilung zu erkennen), nutzen wir (unterschiedlich verteilte) synthetische Daten zum Training. Fitten die resultierenden Netzwerke am Ende gut, wurde eine passende Verteilung gewählt.\n",
    "\n",
    "Aufgrunddessen, dass die Berge nur in Kästen separiert sind und nicht ordentlich ausgeschnitten wurden, werden Überhänge anderer Berge zu finden sein. Wir modellieren dies, in dem zufällig generierte weitere Berge außerhalb des Intervalls hinzu kommen.\n",
    "\n",
    "Sollte dies gar nicht klappen, bleiben uns zwei Möglichkeiten:\n",
    "1. Wir betrachten den Berg als ganzes und wollen dessen Verteilung herausfinden\n",
    "2. Mit Hilfe eines GANs ändern wir die Daten so ab, dass der Diskriminator nicht mehr zwischen echten und synthetischen Daten unterscheiden kann. Fragestellung: Wie kriegen wir Rückfluss in die Parameter? Wahrscheinlich müssen Momente berechnet werden\n",
    "\n",
    "Wir versuchen folgende Netzwerktypen: Feed Forward, LSTM, Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Trainingsdatenstruktur\n",
    "Jeder Trainingsvektor hat den höchsten Wert am Anfang stehen, darauf folgend sind die Nachbarn nach Abstand von rechts zuerst angeordnet. Ist also der höchste Punkt an 3. Stelle, wird aus dem Berg mit x-Achse 1-8 die umverteilte Version 3-4-2-5-1-6-7-8.\n",
    "\n",
    "Jeder Datenpunkt enthält: (corrected) intensity - Abstand (mit Vorzeichen) zu höchstem Punkt - Maske\n",
    "\n",
    "Da in einem Trainingssample zu wenig Punkte sein können, maskieren wir, ob der Datenpunkt genutzt wird (1) oder ob es zu zero-padding kam (0).\n",
    "\n",
    "Außerhalb von LSTMs verwenden wir natürlich eine fixe Vektorlänge, welche aufgrund unserer Ordnung regulierend eingreifen sollte (da nur Tails weggeschnitten werden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_vector_to_training_vector(input_x, input_y, input_length = 500, flatten = False):\n",
    "    # Init: Nullvektoren und Paddingmaske bilden\n",
    "    raw_length = max(len(input_x), input_length)\n",
    "    intensities = np.zeros(raw_length)\n",
    "    abstaende = np.zeros(raw_length)\n",
    "    padding_mask = np.zeros(raw_length)\n",
    "\n",
    "    padding_mask[0:len(input_x)] = 1\n",
    "\n",
    "    # 1. Ort des größten Wertes finden\n",
    "    max_index = np.argmax(input_y)\n",
    "    input_max = input_x[max_index]\n",
    "\n",
    "    # 2. Einsortieren der restlichen Punkte und Berechnung der Abstände\n",
    "    intensities[0] = input_y[max_index]\n",
    "    abstaende[0] = 0\n",
    "\n",
    "    if max_index == 0:\n",
    "        intensities[1:len(input_x)] = input_y[1:]\n",
    "        abstaende[1:len(input_x)] = input_x[1:] - input_max\n",
    "    elif max_index < len(input_x) - max_index: # rechts von max_index sind mehr Punkte\n",
    "        # rechte Seite\n",
    "        intensities[1:2*max_index:2] = input_y[max_index+1:2*max_index+1]\n",
    "        abstaende[1:2*max_index:2] = input_x[max_index+1:2*max_index+1] - input_max\n",
    "        # Tail\n",
    "        intensities[2*max_index+1:len(input_x)] = input_y[2*max_index+1:]\n",
    "        abstaende[2*max_index+1:len(input_x)] = input_x[2*max_index+1:] - input_max\n",
    "        # linke Seite\n",
    "        intensities[2:2*max_index+1:2] = input_y[max_index-1::-1]\n",
    "        abstaende[2:2*max_index+1:2] = input_x[max_index-1::-1] - input_max\n",
    "    else: # links von max_index sind mehr Punkte\n",
    "        # rechte Seite\n",
    "        intensities[1:2*(len(input_x)-max_index)-1:2] = input_y[max_index+1:]\n",
    "        abstaende[1:2*(len(input_x)-max_index)-1:2] = input_x[max_index+1:] - input_max\n",
    "        # linke Seite\n",
    "        intensities[2:2*(len(input_x)-max_index):2] = input_y[max_index-1:2*max_index-len(input_x):-1]\n",
    "        abstaende[2:2*(len(input_x)-max_index):2] = input_x[max_index-1:2*max_index-len(input_x):-1] - input_max\n",
    "        # Tail\n",
    "        intensities[2*(len(input_x)-max_index)-1:len(input_x)] = input_y[2*max_index-len(input_x)::-1]\n",
    "        abstaende[2*(len(input_x)-max_index)-1:len(input_x)] = input_x[2*max_index-len(input_x)::-1] - input_max\n",
    "    \n",
    "    # 3. Ausgabevektor erstellen\n",
    "    if input_length < raw_length:\n",
    "        intensities = intensities[0:input_length]\n",
    "        abstaende = abstaende[0:input_length]\n",
    "        padding_mask = padding_mask[0:input_length]\n",
    "        \n",
    "    if flatten:\n",
    "        output_vector = np.concatenate((intensities, abstaende, padding_mask))\n",
    "    else:\n",
    "        output_vector = [intensities, abstaende, padding_mask]\n",
    "\n",
    "    return output_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generierung synthetischer Daten\n",
    "\n",
    "1. Genutzte Verteilungen\n",
    "2. Einlesen von Peakdaten aus der Datenbank\n",
    "3. Sample Funktion in den Punkten eines Peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Genutzte Verteilungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beobachtungen von Decker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalverteilung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def normal(x, height, offset, mu, sigma, *kwargs):\n",
    "    return offset + height*norm.pdf(x, loc=mu, scale=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponentially modified Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import exponnorm\n",
    "\n",
    "def exp_normal(x, height, offset, mu, sigma, _lambda, *kwargs):\n",
    "    K = 1/(sigma*_lambda)\n",
    "    return offset + height*exponnorm.pdf(x, K, loc=mu, scale=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Allgemeinere adaptive Verteilungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearsonverteilung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearson3\n",
    "\n",
    "def pearson(x, height, offset, mu, sigma, skew, *kwargs):\n",
    "    return offset + height*pearson3.pdf(x, skew, loc=mu, scale=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Johnsons $S_U$-Verteilung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import johnsonsu\n",
    "\n",
    "def johnson(x, height, offset, mu, sigma, a, b, *kwargs):\n",
    "    return offset + height*johnsonsu.pdf(x, a, b, loc=mu, scale=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metalog-Verteilung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metalog(x, height, offset, a, bounds=(-np.inf, np.inf), *kwargs):\n",
    "    lower_bound, upper_bound = bounds\n",
    "\n",
    "    # Skalierung für beschränkte Verteilungen\n",
    "    if np.isfinite(lower_bound) and np.isfinite(upper_bound):\n",
    "        y = (x - lower_bound) / (upper_bound - lower_bound)\n",
    "    else:\n",
    "        y = x\n",
    "\n",
    "    # Berechnung der logit-Transformation\n",
    "    z = np.log(y / (1 - y)) if (np.isfinite(lower_bound) and np.isfinite(upper_bound)) else y\n",
    "\n",
    "    # Berechnung der Metalog-CDF\n",
    "    cdf = a[0] + a[1] * z + a[2] * z**2 + a[3] * z**3 if len(a) >= 4 else 0\n",
    "    for i in range(4, len(a)):\n",
    "        cdf += a[i] * z**i\n",
    "\n",
    "    # Berechnung der Ableitung (PDF)\n",
    "    deriv_z = a[1] + 2 * a[2] * z + 3 * a[3] * z**2\n",
    "    for i in range(4, len(a)):\n",
    "        deriv_z += i * a[i] * z**(i - 1)\n",
    "\n",
    "    if np.isfinite(lower_bound) and np.isfinite(upper_bound):\n",
    "        pdf = deriv_z * (1 / ((y * (1 - y)) * (upper_bound - lower_bound)))\n",
    "    else:\n",
    "        pdf = deriv_z\n",
    "\n",
    "    return offset + height*pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(name):\n",
    "    return {\n",
    "        'normal': normal,\n",
    "        'exp_normal': exp_normal,\n",
    "        'pearson': pearson,\n",
    "        'johnson': johnson,\n",
    "        'metalog': metalog\n",
    "    }[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Einlesen von Peakdaten aus der Datenbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "\n",
    "name_dataset = \"testdata\"\n",
    "\n",
    "engine = create_engine('sqlite:///../Databases/%s.db' % name_dataset) # drei Backslash für relativen Pfad\n",
    "\n",
    "# get all peak_region_number\n",
    "with engine.connect() as conn:\n",
    "    try:\n",
    "        peak_region_number = conn.execute(text(\"SELECT DISTINCT peak_region FROM %s\" % name_dataset)).fetchall()\n",
    "        peak_region_number = [x[0] for x in peak_region_number]\n",
    "    except:\n",
    "        print(\"Keine Verbindung möglich. Datenbank existiert nicht oder ist leer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_peak_region(peak_region_number):\n",
    "    with engine.connect() as conn:\n",
    "        try:\n",
    "            peak_region = conn.execute(text(\"SELECT * FROM %s WHERE peak_region = %d\" % (name_dataset, peak_region_number))).fetchall()\n",
    "            peak_region = [x for x in peak_region]\n",
    "        except:\n",
    "            print(\"Keine Verbindung möglich. Datenbank existiert nicht oder ist leer.\")\n",
    "    return peak_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_peak_region_to_df(peak_region):\n",
    "    peak_region_array = np.asarray(peak_region)\n",
    "    peak_region_df = pd.DataFrame(peak_region_array, columns = [\"index\",\"rt_values\", \"frame_indices\", \"mz_values\", \"tof_indices\", \"mobility_values\", \"scan_indices\", \"intensity_values\", \"corrected_intensity_values\", \"peak_region\"])\n",
    "    return peak_region_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sample Funktion in den Punkten eines Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_synthetic_data(size, distribution, axis, neighbours = False, noise = 0.0, flatten = True):\n",
    "    # Generierung von zufälligen Parametern\n",
    "    height = 100*np.random.rand()\n",
    "    offset = 1e6*np.random.rand()\n",
    "    if distribution == \"normal\":\n",
    "        mu = 1000*np.random.rand()\n",
    "        sigma = 1000*np.random.rand()\n",
    "        parameters = [height, offset, mu, sigma]\n",
    "    elif distribution == \"exp_normal\":\n",
    "        mu = 1000*np.random.rand()\n",
    "        sigma = 1000*np.random.rand()\n",
    "        _lambda = 10*np.random.rand()\n",
    "        parameters = [height, offset, mu, sigma, _lambda]\n",
    "    elif distribution == \"pearson\":\n",
    "        mu = 1000*np.random.rand()\n",
    "        sigma = 1000*np.random.rand()\n",
    "        skew = 10*np.random.rand()\n",
    "        parameters = [height, offset, mu, sigma, skew]\n",
    "    elif distribution == \"johnson\":\n",
    "        mu = 1000*np.random.rand()\n",
    "        sigma = 1000*np.random.rand()\n",
    "        a = 10*np.random.rand()\n",
    "        b = 10*np.random.rand()\n",
    "        parameters = [height, offset, mu, sigma, a, b]\n",
    "    elif distribution == \"metalog\":\n",
    "        a = [10*np.random.rand() for i in range(10)]\n",
    "        parameters = [height, offset,*a]\n",
    "    else:\n",
    "        print(\"Verteilung nicht bekannt.\")\n",
    "        \n",
    "    # Generierung von x-Werten\n",
    "    peak_region = get_peak_region(np.random.choice(peak_region_number))\n",
    "    peak_region_df = convert_peak_region_to_df(peak_region)\n",
    "    x = peak_region_df[axis].values\n",
    "\n",
    "    if neighbours:\n",
    "        # TODO: Störung hinzufügen\n",
    "\n",
    "    # Generierung von Intensity-Werten\n",
    "    distribution = get_distribution(distribution)\n",
    "    y = distribution(x, *parameters) # evaluate density function at the x of the peak region (x: e.g. mz_values in peak_region)\n",
    "    y = y + noise*np.random.randn(len(y)) # random noise auf die Trainingsdaten legen\n",
    "    trainings_vector = input_vector_to_training_vector(x, y, input_length = size, flatten = flatten)\n",
    "    \n",
    "    return trainings_vector, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.31998393e+04, 1.31998393e+04, 1.31998391e+04, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00]),\n",
       " [64.77273856520159,\n",
       "  13199.785925433805,\n",
       "  450.0704270569388,\n",
       "  484.34962896743326])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_synthetic_data(500, \"normal\", \"mz_values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training der Netzwerke\n",
    "\n",
    "Wir trainieren in den folgenden Zellen ein Feed-Forward-Netzwerk (natürlicher Kandidat), ein LSTM und ein Transformer mit nachgeschaltetem Feed-Forward-Netzwerk. Hauptfokus liegt aber erst einmal auf das Feed-Forward-Netzwerk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(input_length = 500):\n",
    "    return sample_synthetic_data(input_length, \"normal\", \"mz_values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Feedforward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m      2\u001b[0m   tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m10\u001b[39m, activation\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m,)),  \u001b[38;5;66;03m# input shape required\u001b[39;00m\n\u001b[1;32m      3\u001b[0m   tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m10\u001b[39m, activation\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu),\n\u001b[1;32m      4\u001b[0m   tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m ])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'keras'"
     ]
    }
   ],
   "source": [
    "input_length = 500\n",
    "trainings_vector, parameters = get_sample(input_length)\n",
    "# Preprocessing\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(input_length,)),  # input shape required\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(len(parameters))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss_object \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mMeanSquaredError()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(model, x, y, training):\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;66;03m# training=training is needed only if there are layers with different\u001b[39;00m\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;66;03m# behavior during training versus inference (e.g. Dropout).\u001b[39;00m\n\u001b[1;32m      6\u001b[0m   y_ \u001b[38;5;241m=\u001b[39m model(x, training\u001b[38;5;241m=\u001b[39mtraining)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'keras'"
     ]
    }
   ],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def loss(model, x, y, training):\n",
    "  # training=training is needed only if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  y_ = model(x, training=training)\n",
    "\n",
    "  return loss_object(y_true=y, y_pred=y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'keras'"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "num_epochs = 201\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "\n",
    "  # Training loop - using batches of 32\n",
    "  for _ in range(32):\n",
    "    # Get training sample\n",
    "    x,y = get_sample(input_length)\n",
    "    # Preprocess\n",
    "\n",
    "    # Optimize the model\n",
    "    loss_value, grads = grad(model, x, y)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Track progress\n",
    "    epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n",
    " \n",
    "  # End epoch\n",
    "  train_loss_results.append(epoch_loss_avg.result())\n",
    "\n",
    "  if epoch % 50 == 0:\n",
    "    print(\"Epoch {:03d}: Loss: {:.3f},\".format(epoch, epoch_loss_avg.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model_path = '../Networks/feedforward.keras'\n",
    "model.save(keras_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphatims",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
